usage: training.py [-h] [--epochs EPOCHS] [--mirror MIRROR] [--save_ds SAVE_DS] [--batch BATCH]

Parameters for training model: number of epochs and access to several GPUs

optional arguments:
  -h, --help         show this help message and exit
  --epochs EPOCHS    The number of epoch for training
  --mirror MIRROR    if device has several GPUs then mirror strategy distributes training among it
  --save_ds SAVE_DS  save datasets after creating for speed up in next training
  --batch BATCH      the size of batch during training

Link for download data: [link](https://www.immersivelimit.com/datasets/cigarette-butts).

If you are using Ubuntu 22.04

Доступные данные разделены на несколько папок:  
- `real_test` содержит фотографии 512x512x3;  
- `train/images` содержит фотографии 512x512x3;  
- `train/coco_annotations.json` содержит аннотации в формате [COCO](http://cocodataset.org/#format-data);  
- `val/images` содержит фотографии 512x512x3;  
- `val/coco_annotations.json` содержит аннотации в формате [COCO](http://cocodataset.org/#format-data).

# Результаты

Для лучшей модели требуется создать 2 файла, которые необходимы для валидации Вашего решения:  
- сохраненные маски для картинок из valid в формате pred_valid_template.csv (в архиве с `data`) и залить его с тем же именем (см. `notebooks/GettingStarted.ipynb`);   
- html страницу с предсказанием модели для всех картинок из `real_test` и папку с используемыми картинками в этой html странице для её просмотра. Создать zip файл c html и изображениями и залить его в папку `results` (см. `notebooks/GettingStarted.ipynb`)  
  
Также необходимо:  
- подготовить код (сам репозиторий) для проверки (докстринги, PEP8);  
- создать отчет (можно прямо в ноутбуке) с описанием Вашего исследования, предобработки, постобработки, проверямых гипотез, используемых моделей, описание лучшего подхода и т.п. (он должен лежать в папке `notebooks`);  

# Рекомендуемый pipeline решения:

Предполагается следующий pipeline решения поставленной задачи:  
- `fork` данного репозитория;  
- ознакомиться с критериями;  
- скачать данные;  
- ознакомиться с `notebooks/GettingStarted.ipynb`;  
- ознакомиться с данными, разобраться с их форматом;  
- ознакомиться с базовой [статьей](https://arxiv.org/pdf/1505.04597.pdf);  
- провести анализ данных;  
- написать методы аугментации данных;  
- реализовать нейросеть/нейросети (необязательно с нуля);  
- провалидировать модели;  
- выбрать **лучшую** модель на `val` и посчитать для нее метрики;  
- получить результаты на реальных изображениях `real_test` и сохранить их;  
- проанализировать результаты, сформулировать проблемы модели.  

# Критерии

При оценке решения этой задачи акцент будет делаться на (в порядке приоритета):  
1. Качество исследования в jupyter notebook и чистота кода. В этот критерий входит читаемость и адекватность кода, содержательность комментариев, правильное оформление графиков (если таковые будут), отсутствие смысловых ошибок в коде. Наличие docstrings к написаным функциям приветствуется. Также будет оцениваться демонстрация предпринятых шагов и структурирование кода (разные по смыслу куски кода разделены по пакетам).  
2. Анализ данных и подходы к аугментации. Оригинальность, эффективность и количество идей. Использование неочевидных шагов в решении, которые улучшают качество, будет хорошим плюсом. Наличие нескольких подходов также будет ценится выше, чем один подход.  
3. Значение метрик на контрольной выборке и общая адекватность модели.  
4. Обоснование выбора моделей (процесс выбора моделей).  
5. Анализ результатов итоговой модели.
